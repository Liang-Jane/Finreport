{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "\n",
    "# Import deepeval related modules\n",
    "from deepeval.metrics import GEval\n",
    "from deepeval.test_case import LLMTestCase, LLMTestCaseParams\n",
    "from deepeval.metrics.g_eval import Rubric\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set OpenAI API Key\n",
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \n",
    "                      \"-i\", \"https://pypi.tuna.tsinghua.edu.cn/simple\", \"pymupdf\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Path\n",
    "OUTPUT_DIR = Path(r\"./annual-reports_output\")\n",
    "INPUT_DIR = Path(r\"./annual-reports\")\n",
    "\n",
    "# Evaluation Results Save Directory\n",
    "EVAL_OUTPUT_DIR = OUTPUT_DIR / \"evaluations_geval\"\n",
    "EVAL_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Define Stage Directory\n",
    "STAGE_DIRS = {\n",
    "    \"stage1_baseline\": OUTPUT_DIR / \"stage1_baseline\",\n",
    "    \"stage2_role_background\": OUTPUT_DIR / \"stage2_role_background\",\n",
    "    \"stage3_constraints\": OUTPUT_DIR / \"stage3_constraints\",\n",
    "    \"stage4_cot_laysummary\": OUTPUT_DIR / \"stage4_cot_laysummary\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Evaluation Metrics (Based on the Four Dimensions from the Paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set deepeval timeout\n",
    "os.environ[\"DEEPEVAL_PER_TASK_TIMEOUT_SECONDS\"] = \"300\"\n",
    "os.environ[\"DEEPEVAL_PER_ATTEMPT_TIMEOUT_SECONDS\"] = \"120\"\n",
    "\n",
    "# 1. Numerical Factuality\n",
    "numerical_factuality_metric = GEval(\n",
    "    name=\"Numerical Factuality\",\n",
    "    criteria=\"\"\"Evaluate whether the numerical data in the generated financial summary is consistent with the original annual report (text and embedded tables).\n",
    "                1. Strictly check for \"numerical hallucinations\" or fabricated numbers;\n",
    "                2. Verify that numbers are not incorrectly associated with mismatched indicators;\n",
    "                3. Special attention: If the model converts long numerical strings into public-friendly units (e.g., \"¥2.45 billion\" instead of \"2,450,123,000.00\"), as long as the magnitude is accurate, it should be considered factual and audience-friendly.。\"\"\",\n",
    "    evaluation_params=[\n",
    "        LLMTestCaseParams.ACTUAL_OUTPUT,\n",
    "        LLMTestCaseParams.CONTEXT,\n",
    "    ],\n",
    "    threshold=0.7,\n",
    "    model=\"gpt-3.5-turbo\", # It is recommended to use a higher-performance model as an evaluator to simulate expert logic \n",
    "    async_mode=False,\n",
    "    rubric=[\n",
    "        Rubric(score_range=(0, 3), expected_outcome=\"Obvious numerical errors, false fabrication, or serious indicator misguidance exist\"),\n",
    "        Rubric(score_range=(4, 6), expected_outcome=\"Core numbers are basically correct, but there are individual data deviations or unit conversion errors\"),\n",
    "        Rubric(score_range=(7, 8), expected_outcome=\"Numbers are accurate, highly consistent with the original text, and the data presentation is suitable for public understanding\"),\n",
    "        Rubric(score_range=(9, 10), expected_outcome=\"Numbers are completely accurate with no hallucinations, precisely extracting core financial indicators that support decision-making\"),\n",
    "    ],\n",
    ")\n",
    "\n",
    "# 2. Readability & Accessibility\n",
    "readability_metric = GEval(\n",
    "    criteria=\"\"\"Evaluate whether the summary successfully converts obscure accounting terminology into business language that ordinary readers can understand.\n",
    "    1. Focus on whether analogies or explanatory descriptions are used (e.g., explaining \"goodwill impairment\" as \"the acquired company is worth less than expected\");\n",
    "    2. Simple verbatim extraction from the original text is strictly prohibited; semantic reconstruction must be performed;\n",
    "    3. Language should be concise and barrier-free, ensuring that non-professional investors can understand how the company makes money.\"\"\"\n",
    "   ),\n",
    "    threshold=0.7,\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    async_mode=False,\n",
    "    rubric=[\n",
    "        Rubric(score_range=(0, 3), expected_outcome=\"Filled with accounting terminology, directly extracted from the original text, extremely unfriendly to the general public\"),\n",
    "        Rubric(score_range=(4, 6), expected_outcome=\"Attempts to popularize, but still contains many professional terms, explanations are not straightforward enough\"),\n",
    "        Rubric(score_range=(7, 8), expected_outcome=\"Language is accessible, successfully removed most terminology barriers, suitable for ordinary investors to read\"),\n",
    "        Rubric(score_range=(9, 10), expected_outcome=\"High-level semantic reconstruction, using vivid analogies to resolve professional terminology, with extremely strong popularization and communication value\"),\n",
    "    ],\n",
    ")\n",
    "\n",
    "# 3. Coherence\n",
    "coherence_metric = GEval(\n",
    "    name=\"Coherence\",\n",
    "    criteria=\"\"\"Evaluate whether the summary maintains clear narrative logic when processing financial reports that are hundreds of pages long.\n",
    "    1. Check for logical gaps between paragraphs or information fragmentation caused by insufficient long-text processing capabilities;\n",
    "    2. The summary should follow a clear contextual flow (e.g., business overview -> financial performance -> risk warnings);\n",
    "    3. Ensure there are no contradictions or meaningless repetitive statements [cite: 26].\"\"\",\n",
    "    evaluation_params=[\n",
    "        LLMTestCaseParams.ACTUAL_OUTPUT,\n",
    "    ],\n",
    "    threshold=0.7,\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    async_mode=False,\n",
    "    rubric=[\n",
    "        Rubric(score_range=(0, 3), expected_outcome=\"Extremely chaotic logic, severe information fragmentation, readers cannot connect the company's overall operations\"),\n",
    "        Rubric(score_range=(4, 6), expected_outcome=\"Basically coherent, but some sections have awkward transitions, with certain logical jumps\"),\n",
    "        Rubric(score_range=(7, 8), expected_outcome=\"Clear logic, rigorous structure, successfully condensed massive long text into a coherent guide\"),\n",
    "        Rubric(score_range=(9, 10), expected_outcome=\"Extremely fluent narrative, impeccable contextual logic, demonstrating strong cross-chapter data correlation capabilities\"),\n",
    "    ],\n",
    ")\n",
    "\n",
    "# 4. Informativeness\n",
    "informativeness_metric = GEval(\n",
    "    name=\"Informativeness\",\n",
    "    criteria=\"\"\"Verify whether the summary completely covers core sections such as balance sheet, income statement, and risk factors.\n",
    "    1. Must include core operational information sufficient to support public investment decisions;\n",
    "    2. Focus on checking whether \"negative indicators\" and \"risk warnings\" are mandatorily retained to ensure the summary is objective and neutral;\n",
    "    3. The evaluation standard is whether \"simplified without losing key points\" has been achieved, not omitting major financial changes while reducing length.\"\"\"\n",
    "    evaluation_params=[\n",
    "        LLMTestCaseParams.ACTUAL_OUTPUT,\n",
    "        LLMTestCaseParams.CONTEXT,\n",
    "    ],\n",
    "    threshold=0.7,\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    async_mode=False,\n",
    "    rubric=[\n",
    "        Rubric(score_range=(0, 3), expected_outcome=\"Severe information deficiency, missing key sections such as income statement or risk warnings\"),\n",
    "        Rubric(score_range=(4, 6), expected_outcome=\"Covers basic data, but extraction of risk factors or business details is not comprehensive enough\"),\n",
    "        Rubric(score_range=(7, 8), expected_outcome=\"Complete content, comprehensively covers the three core financial statements and operational risks, with moderate information volume\"),\n",
    "        Rubric(score_range=(9, 10), expected_outcome=\"Extremely complete and well-prioritized, conveying the most decision-valuable financial information in a very short length\"),\n",
    "    ],\n",
    ")\n",
    "all_metrics = [numerical_factuality_metric, readability_metric, coherence_metric, informativeness_metric]\n",
    "print(f\"Defined {len(all_metrics)} evaluation metrics (based on the four dimensions from the paper)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # pymupdf\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "def pdf_to_text(pdf_path: str) -> str:\n",
    "    \"\"\"Extract text from PDF\"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    parts = []\n",
    "    for page in doc:\n",
    "        t = page.get_text(\"text\")\n",
    "        if t and t.strip():\n",
    "            parts.append(t)\n",
    "    return \"\\n\\n\".join(parts)\n",
    "\n",
    "def clean_text(t: str) -> str:\n",
    "    \"\"\"Clean text\"\"\"\n",
    "    t = unicodedata.normalize(\"NFKC\", t)\n",
    "    t = re.sub(r\"[\\x00-\\x08\\x0b\\x0c\\x0e-\\x1f]\", \" \", t)\n",
    "    t = \"\\n\".join(line for line in t.splitlines() if not re.fullmatch(r\"\\s*\\d+\\s*\", line))\n",
    "    t = re.sub(r\"\\n{3,}\", \"\\n\\n\", t)\n",
    "    return t\n",
    "\n",
    "def get_original_text(report_file: Path) -> str:\n",
    "    \"\"\"Find the corresponding original PDF based on the report filename and extract text\"\"\"\n",
    "    # Extract original filename from report filename (remove _short_report.txt suffix)\n",
    "    pdf_name = report_file.stem.replace(\"_short_report\", \"\") + \".pdf\"\n",
    "    pdf_path = INPUT_DIR / pdf_name\n",
    "    \n",
    "    if pdf_path.exists():\n",
    "        text = pdf_to_text(str(pdf_path))\n",
    "        return clean_text(text)\n",
    "    else:\n",
    "        print(f\"Warning: Corresponding PDF file not found: {pdf_path}\")\n",
    "        return \"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute Evaluation (Supports Multiple Stages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate reports from all stages\n",
    "all_evaluation_results = []\n",
    "\n",
    "for stage_name, stage_dir in STAGE_DIRS.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Starting evaluation for stage: {stage_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    if not stage_dir.exists():\n",
    "        print(f\"Warning: Stage directory does not exist: {stage_dir}\")\n",
    "        continue\n",
    "    \n",
    "    # Get all report files for this stage\n",
    "    report_files = sorted(stage_dir.glob(\"*_short_report.txt\"))\n",
    "    print(f\"Found {len(report_files)} report files\")\n",
    "    \n",
    "    for report_file in tqdm(report_files, desc=f\"Evaluating {stage_name}\"):\n",
    "        # Read the generated report\n",
    "        actual_output = report_file.read_text(encoding=\"utf-8\")\n",
    "        \n",
    "        # Read original annual report text as context\n",
    "        original_text = get_original_text(report_file)\n",
    "        \n",
    "        # If original text is too long, truncate to first 50,000 characters\n",
    "        if len(original_text) > 50000:\n",
    "            original_text = original_text[:50000] + \"\\n\\n[Text truncated...]\"\n",
    "        \n",
    "        # Create test case\n",
    "        context_list = [original_text] if original_text else None\n",
    "        test_case = LLMTestCase(\n",
    "            input=f\"Evaluate report file: {report_file.name}\",\n",
    "            actual_output=actual_output,\n",
    "            context=context_list,\n",
    "        )\n",
    "        \n",
    "        # Evaluate each metric\n",
    "        result = {\n",
    "            \"stage\": stage_name,\n",
    "            \"file\": report_file.name,\n",
    "            \"report_length\": len(actual_output),\n",
    "        }\n",
    "        \n",
    "        for metric in all_metrics:\n",
    "            try:\n",
    "                score = metric.measure(test_case, _show_indicator=True)\n",
    "                result[f\"{metric.name}_score\"] = score\n",
    "                result[f\"{metric.name}_success\"] = metric.success\n",
    "                result[f\"{metric.name}_reason\"] = metric.reason\n",
    "                \n",
    "                if hasattr(metric, 'evaluation_cost') and metric.evaluation_cost:\n",
    "                    result[f\"{metric.name}_cost\"] = metric.evaluation_cost\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error evaluating {metric.name} ({report_file.name}): {e}\")\n",
    "                result[f\"{metric.name}_score\"] = None\n",
    "                result[f\"{metric.name}_success\"] = False\n",
    "                result[f\"{metric.name}_reason\"] = f\"Error: {str(e)}\"\n",
    "        \n",
    "        all_evaluation_results.append(result)\n",
    "        \n",
    "        # Save detailed evaluation results for a single report\n",
    "        eval_file = EVAL_OUTPUT_DIR / f\"{stage_name}_{report_file.stem}_evaluation.json\"\n",
    "        with open(eval_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(result, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"\\nEvaluation completed, evaluated {len(all_evaluation_results)} reports\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Summary and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame\n",
    "df_results = pd.DataFrame(all_evaluation_results)\n",
    "\n",
    "# Display all columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "print(\"Evaluation Results Summary:\")\n",
    "print(df_results.to_string())\n",
    "\n",
    "# Save as CSV\n",
    "csv_file = EVAL_OUTPUT_DIR / \"evaluation_summary.csv\"\n",
    "df_results.to_csv(csv_file, index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"\\nResults saved to: {csv_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View detailed evaluation results for a report\n",
    "if len(all_evaluation_results) > 0:\n",
    "    first_result = all_evaluation_results[0]\n",
    "    print(f\"\\nReport: {first_result['file']} (Stage: {first_result['stage']})\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for metric in all_metrics:\n",
    "        metric_name = metric.name\n",
    "        score = first_result.get(f\"{metric_name}_score\")\n",
    "        reason = first_result.get(f\"{metric_name}_reason\", \"None\")\n",
    "        \n",
    "        print(f\"\\n【{metric_name}】\")\n",
    "        print(f\"Score: {score}\")\n",
    "        print(f\"Evaluation Reason: {reason}\")\n",
    "        print(\"-\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate statistics\n",
    "score_columns = [col for col in df_results.columns if col.endswith(\"_score\")]\n",
    "\n",
    "print(\"\\nAverage scores for all metrics across all stages:\")\n",
    "for col in score_columns:\n",
    "    metric_name = col.replace(\"_score\", \"\")\n",
    "    avg_score = df_results[col].mean()\n",
    "    print(f\"{metric_name}: {avg_score:.3f}\")\n",
    "\n",
    "print(\"\\nPass rates for each metric (>= threshold):\")\n",
    "success_columns = [col for col in df_results.columns if col.endswith(\"_success\")]\n",
    "for col in success_columns:\n",
    "    metric_name = col.replace(\"_success\", \"\")\n",
    "    pass_rate = df_results[col].sum() / len(df_results) * 100\n",
    "    print(f\"{metric_name}: {pass_rate:.1f}%\")\n",
    "\n",
    "# Statistics by stage\n",
    "print(\"\\nAverage score comparison by stage:\")\n",
    "if 'stage' in df_results.columns:\n",
    "    stage_stats = df_results.groupby('stage')[score_columns].mean()\n",
    "    print(stage_stats.to_string())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}